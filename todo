read other similar paper, just to get an idea


does it work with reward network?
plot with different number of NNs and see how performance change
keep update in memory (different dataset between different NN) to increase stability. Make it optional (on policy - off policy). does it improve? 

codice per le routine di estimation of the different parts


questions: 
why if I put a minus in front the policy loss goes up and Q values go down, but it still work???
why changing eta only changes the value of the loss but doesn't increase or dicrease the exploration rate??
Why does it run faster on my mac compared to cluster? I think because there isn't a lot of stuff to paralelize